{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3de34f29-8e37-4305-a027-862cc40ff9be",
   "metadata": {},
   "source": [
    "## STEP 1: Brand Mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cb5433f-fd99-4940-b82a-bbb4d5314373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== VALIDATING INPUT FILES ===\n",
      "âœ“ Input files validated successfully\n",
      "\n",
      "=== BUILDING NUTRACEUTICALS BRAND CATALOG ===\n",
      "âœ“ Loaded 544 brands from file\n",
      "âœ“ Enhanced with 7 predefined brand mappings\n",
      "âœ“ Loaded 544 product descriptions\n",
      "âœ“ Loaded 545 canonical brands\n",
      "âœ“ Built 1060 exact match patterns\n",
      "âœ“ Built 1060 word-to-brand mappings\n",
      "âœ“ Built 1579 regex patterns\n",
      "âœ“ Enhanced with 7 predefined mappings\n",
      "\n",
      "=== LOADING NUTRACEUTICALS SEARCH DATA ===\n",
      "âœ“ Loaded 15249 search queries\n",
      "\n",
      "=== PROCESSING DATA ===\n",
      "  -> Processing chunk 1 (1000 rows)...\n",
      "  -> Mapping brands for 1000 queries...\n",
      "    -> Using simple clustering for small dataset (338 rows)\n",
      "  -> Processing chunk 2 (1000 rows)...\n",
      "  -> Mapping brands for 1000 queries...\n",
      "    -> Using simple clustering for small dataset (473 rows)\n",
      "  -> Processing chunk 3 (1000 rows)...\n",
      "  -> Mapping brands for 1000 queries...\n",
      "    -> Using simple clustering for small dataset (565 rows)\n",
      "  -> Processing chunk 4 (1000 rows)...\n",
      "  -> Mapping brands for 1000 queries...\n",
      "    -> Using simple clustering for small dataset (629 rows)\n",
      "  -> Processing chunk 5 (1000 rows)...\n",
      "  -> Mapping brands for 1000 queries...\n",
      "    -> Using simple clustering for small dataset (704 rows)\n",
      "  -> Processing chunk 6 (1000 rows)...\n",
      "  -> Mapping brands for 1000 queries...\n",
      "    -> Using simple clustering for small dataset (720 rows)\n",
      "  -> Processing chunk 7 (1000 rows)...\n",
      "  -> Mapping brands for 1000 queries...\n",
      "    -> Using simple clustering for small dataset (749 rows)\n",
      "  -> Processing chunk 8 (1000 rows)...\n",
      "  -> Mapping brands for 1000 queries...\n",
      "    -> Using simple clustering for small dataset (766 rows)\n",
      "  -> Processing chunk 9 (1000 rows)...\n",
      "  -> Mapping brands for 1000 queries...\n",
      "    -> Using simple clustering for small dataset (794 rows)\n",
      "  -> Processing chunk 10 (1000 rows)...\n",
      "  -> Mapping brands for 1000 queries...\n",
      "    -> Using simple clustering for small dataset (833 rows)\n",
      "  -> Processing chunk 11 (1000 rows)...\n",
      "  -> Mapping brands for 1000 queries...\n",
      "    -> Using simple clustering for small dataset (831 rows)\n",
      "  -> Processing chunk 12 (1000 rows)...\n",
      "  -> Mapping brands for 1000 queries...\n",
      "    -> Using simple clustering for small dataset (863 rows)\n",
      "  -> Processing chunk 13 (1000 rows)...\n",
      "  -> Mapping brands for 1000 queries...\n",
      "    -> Using simple clustering for small dataset (856 rows)\n",
      "  -> Processing chunk 14 (1000 rows)...\n",
      "  -> Mapping brands for 1000 queries...\n",
      "    -> Using simple clustering for small dataset (846 rows)\n",
      "  -> Processing chunk 15 (1000 rows)...\n",
      "  -> Mapping brands for 1000 queries...\n",
      "    -> Using simple clustering for small dataset (888 rows)\n",
      "  -> Processing chunk 16 (249 rows)...\n",
      "  -> Mapping brands for 249 queries...\n",
      "    -> Using simple clustering for small dataset (239 rows)\n",
      "âœ“ Method: simple_identity\n",
      "âœ“ Created 888 clusters\n",
      "âœ“ Top brands: {'Other': np.int64(13080), 'NOW Foods': np.int64(93), 'CENTRUM': np.int64(83), 'VITABIOTICS': np.int64(76), 'ARGIVIT': np.int64(68), 'Manuka Health': np.int64(56), 'Holista': np.int64(45), 'Solgar': np.int64(41), 'Natrol': np.int64(29), 'Neocell': np.int64(23)}\n",
      "\n",
      "=== SAVING RESULTS ===\n",
      "âœ“ Results saved to: NUTRACEUTICALS_Enhanced_with_Brands_and_Clusters.xlsx (enhanced_data sheet only)\n",
      "\n",
      "=== FINAL STATISTICS ===\n",
      "ðŸ“Š Total queries processed: 15,249\n",
      "ðŸ“Š Total clusters created: 888\n",
      "ðŸ“Š Brands identified: 253\n",
      "\n",
      "=== BRAND MAPPING RESULTS ===\n",
      "ðŸ·ï¸  Branded queries: 2,169 (14.2%)\n",
      "â“ Unclassified (Other): 13,080 (85.8%)\n",
      "\n",
      "ðŸ† Top 15 brands by query count:\n",
      "   1. Other               : 13,080 queries ( 85.8%) | 4,272,464 searches\n",
      "   2. NOW Foods           :   93 queries (  0.6%) |   41,099 searches\n",
      "   3. CENTRUM             :   83 queries (  0.5%) |   86,668 searches\n",
      "   4. VITABIOTICS         :   76 queries (  0.5%) |   60,953 searches\n",
      "   5. ARGIVIT             :   68 queries (  0.4%) |   69,265 searches\n",
      "   6. Manuka Health       :   56 queries (  0.4%) |   39,845 searches\n",
      "   7. Holista             :   45 queries (  0.3%) |   16,611 searches\n",
      "   8. Solgar              :   41 queries (  0.3%) |   10,281 searches\n",
      "   9. Natrol              :   29 queries (  0.2%) |    6,331 searches\n",
      "  10. Neocell             :   23 queries (  0.2%) |    5,428 searches\n",
      "  11. Foods Alive         :   22 queries (  0.1%) |    8,411 searches\n",
      "  12. Vital Proteins      :   22 queries (  0.1%) |    4,080 searches\n",
      "  13. Ensure Max          :   22 queries (  0.1%) |   95,054 searches\n",
      "  14. Dymatize            :   21 queries (  0.1%) |    4,301 searches\n",
      "  15. SOLARAY             :   19 queries (  0.1%) |    3,943 searches\n",
      "\n",
      "=== SEARCH VOLUME STATISTICS ===\n",
      "ðŸ“ˆ Total search volume: 5,089,152\n",
      "ðŸ“ˆ Average searches per query: 333.7\n",
      "\n",
      "ðŸ” Top 10 queries by search volume:\n",
      "   1. ensure max protein             [Ensure Max     ]: 67,910 searches\n",
      "   2. Ú©ÙˆÙ„Ø§Ø¬ÛŒÙ†                        [Other          ]: 37,012 searches\n",
      "   3. Ú©ÙˆÙ„Ø§Ø¬ÛŒÙ†                        [Other          ]: 35,799 searches\n",
      "   4. ÙÛŒØªØ§Ù…ÛŒÙ† Ø³ÛŒ                     [Other          ]: 28,423 searches\n",
      "   5. Ú©ÙˆÙ„Ø§Ø¬ÛŒÙ†                        [Other          ]: 27,780 searches\n",
      "   6. ÙÛŒØªØ§Ù…ÛŒÙ† Ø³ÛŒ                     [Other          ]: 23,298 searches\n",
      "   7. Ù…ÛŒÙ„Ø§ØªÙˆÙ†ÛŒÙ†                      [Other          ]: 22,055 searches\n",
      "   8. ÙÛŒØªØ§Ù…ÛŒÙ† Ø¯                      [Other          ]: 21,953 searches\n",
      "   9. Ù…ÛŒÙ„Ø§ØªÙˆÙ†ÛŒÙ†                      [Other          ]: 20,978 searches\n",
      "  10. Ù…ØºÙ†ÛŒØ³ÛŒÙˆÙ…                       [Other          ]: 20,298 searches\n",
      "\n",
      "=== CLUSTER ANALYSIS ===\n",
      "ðŸ“Š Average cluster size: 17.2 queries\n",
      "ðŸ“Š Largest cluster: 38 queries\n",
      "ðŸ“Š Smallest cluster: 1 queries\n",
      "\n",
      "ðŸ“Š Top brands by cluster count:\n",
      "  Other               : 882 clusters | 13080 queries | avg size: 14.8\n",
      "  CENTRUM             :  56 clusters |   83 queries | avg size: 1.5\n",
      "  NOW Foods           :  54 clusters |   93 queries | avg size: 1.7\n",
      "  VITABIOTICS         :  39 clusters |   76 queries | avg size: 1.9\n",
      "  ARGIVIT             :  36 clusters |   68 queries | avg size: 1.9\n",
      "  Solgar              :  31 clusters |   41 queries | avg size: 1.3\n",
      "  Holista             :  31 clusters |   45 queries | avg size: 1.5\n",
      "  Manuka Health       :  30 clusters |   56 queries | avg size: 1.9\n",
      "  Dymatize            :  19 clusters |   21 queries | avg size: 1.1\n",
      "  Vital Proteins      :  17 clusters |   22 queries | avg size: 1.3\n",
      "\n",
      "ðŸŽ‰ Processing completed successfully!\n",
      "ðŸ“ Output file: NUTRACEUTICALS_Enhanced_with_Brands_and_Clusters.xlsx\n",
      "     - enhanced_data: Main data with Brand column and Cluster ID\n",
      "\n",
      "ðŸ“‹ Column sequence in enhanced_data:\n",
      "     category | Brand | search | count | Clicks | Conversions | clickThroughRate | conversionRate | averageClickPosition | underperforming | start_date | end_date | Cluster ID\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import warnings\n",
    "import unicodedata\n",
    "from typing import List, Tuple, Dict\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Quick fix for MKL/sklearn crashes\n",
    "os.environ['MKL_THREADING_LAYER'] = 'GNU'\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "CONFIG = {\n",
    "    \"input_file\": \"NUTRACEUTICALS AND NUTRITION combined_data_ June - August 2025.xlsx\",\n",
    "    \"brands_file\": \"Brands.xlsx\",\n",
    "    \"output_file\": \"NUTRACEUTICALS_Enhanced_with_Brands_and_Clusters.xlsx\",\n",
    "    \"use_dedup\": True,\n",
    "    \"distance_threshold\": 0.50,\n",
    "    \"embedding_model\": \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",\n",
    "    \"fallback_fuzzy_threshold\": 70,\n",
    "    \"max_top_keywords_per_cluster\": 12,\n",
    "    \"embedding_batch_size\": 64,\n",
    "    \"brand_cluster_min_size\": 3,\n",
    "    \"brand_similarity_threshold\": 0.6,\n",
    "    \"high_fuzzy_threshold\": 90,  # Increased for stricter matching\n",
    "    \"med_fuzzy_threshold\": 85,   # Increased for stricter matching\n",
    "    \"low_fuzzy_threshold\": 80,   # Increased for stricter matching\n",
    "    \"min_brand_length\": 3,\n",
    "    \"simple_clustering_threshold\": 1000,\n",
    "    \"chunk_size\": 1000,\n",
    "    \"exact_match_threshold\": 95,  # New: for very strict exact matching\n",
    "    \"partial_match_penalty\": 0.3,  # New: penalty for partial matches\n",
    "    \"min_confidence_threshold\": 0.75  # New: minimum confidence to accept match\n",
    "}\n",
    "\n",
    "# ---------------- NORMALIZATION FUNCTIONS ----------------\n",
    "\n",
    "ARABIC_VARIATIONS = {\n",
    "    'Ø£': 'Ø§', 'Ø¥': 'Ø§', 'Ø¢': 'Ø§', 'Ù±': 'Ø§',\n",
    "    'ÙŠ': 'ÙŠ', 'Ù‰': 'ÙŠ', 'Ø¦': 'ÙŠ', 'Ø¤': 'Ùˆ',\n",
    "    'Ø©': 'Ù‡',\n",
    "    'Ù¾': 'Ø¨', 'Ú†': 'Ø¬', 'Ú˜': 'Ø²', 'Ú¯': 'Ùƒ', 'Ú©': 'Ùƒ',\n",
    "    'Ùƒ': 'Ùƒ', 'Ú©': 'Ùƒ',\n",
    "    'ÛŒ': 'ÙŠ', 'ÙŠ': 'ÙŠ'\n",
    "}\n",
    "\n",
    "ALNUM_PATTERN = re.compile(r\"[^0-9a-zA-Z\\u0600-\\u06FF\\u0750-\\u077F]+\", re.UNICODE)\n",
    "\n",
    "def enhanced_arabic_normalize(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    for old, new in ARABIC_VARIATIONS.items():\n",
    "        text = text.replace(old, new)\n",
    "    text = re.sub(r'[\\u064B-\\u0652\\u0670\\u0640]', '', text)\n",
    "    return text\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = enhanced_arabic_normalize(text)\n",
    "    text = text.strip().lower()\n",
    "    text = ALNUM_PATTERN.sub(\" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "def tight_normalize(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = enhanced_arabic_normalize(text)\n",
    "    text = text.lower()\n",
    "    text = ALNUM_PATTERN.sub(\"\", text)\n",
    "    return text\n",
    "\n",
    "def enhanced_clustering_normalize(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = normalize_text(s)\n",
    "    s = re.sub(r'\\b(for|with|and|or|&|mg|g|ml|oz|units?|u|gm)\\b', ' ', s)\n",
    "    s = re.sub(r'\\b(size|pack|piece|pcs|box|tin|can|container|vial)\\b', ' ', s)\n",
    "    s = re.sub(r'\\b(stage|step|phase|level|new)\\b', ' ', s)\n",
    "    s = re.sub(r'\\d+(\\.\\d+)?', ' NUM ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    return s\n",
    "\n",
    "# ---------------- IMPORT OPTIONAL LIBRARIES ----------------\n",
    "\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    EMBEDDINGS_OK = True\n",
    "except ImportError:\n",
    "    EMBEDDINGS_OK = False\n",
    "\n",
    "try:\n",
    "    from rapidfuzz import fuzz, process\n",
    "    USE_RAPIDFUZZ = True\n",
    "except ImportError:\n",
    "    import difflib\n",
    "    USE_RAPIDFUZZ = False\n",
    "\n",
    "try:\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    from sklearn.cluster import AgglomerativeClustering\n",
    "except ImportError:\n",
    "    TfidfVectorizer = None\n",
    "    AgglomerativeClustering = None\n",
    "\n",
    "# ---------------- ENHANCED BRAND MAPPING ----------------\n",
    "\n",
    "def create_enhanced_brand_mapping():\n",
    "    \"\"\"Create comprehensive brand mapping with Arabic names and product lines\"\"\"\n",
    "    \n",
    "    # Enhanced brand mapping with Arabic transliterations and product lines\n",
    "    enhanced_mapping = {\n",
    "        # ARGIVIT - Main brand with product lines\n",
    "        \"ARGIVIT\": [\n",
    "            \"argivit\", \"Ø§Ø±Ø¬ÙŠÙÙŠØª\", \"Ø§Ø±Ø¬ÛŒÙØª\", \"Ø§Ø±Ø¬ÙÛŒØª\", \"Ø§Ø±Ø¬ÛŒÙØª\",\n",
    "            \"argivit classic\", \"argivit focus\", \"argivit immune\",\n",
    "            \"argivit smart\", \"argivit growth\"\n",
    "        ],\n",
    "        \n",
    "        # VITABIOTICS - Include Arabic names\n",
    "        \"VITABIOTICS\": [\n",
    "            \"vitabiotics\", \"ÙÛŒØªØ§Ø¨ÛŒÙˆØªÚ©Ø³\", \"ÙÙŠØªØ§Ø¨ÙŠÙˆØªÙƒØ³\",\n",
    "            \"osteocare\", \"feroglobin\", \"ÙÛŒØ±ÙˆØ¬Ù„ÙˆØ¨ÛŒÙ†\", \"ÙÙŠØ±ÙˆØ¬Ù„ÙˆØ¨ÙŠÙ†\",\n",
    "            \"pregnacare\", \"wellwoman\", \"wellman\"\n",
    "        ],\n",
    "        \n",
    "        # CENTRUM - Include Arabic variations\n",
    "        \"CENTRUM\": [\n",
    "            \"centrum\", \"Ø³Ù†ØªØ±ÙˆÙ…\", \"Ø³ÛŒÙ†ØªØ±ÙˆÙ…\", \"Ø³Ù†ØªØ±Ù…\"\n",
    "        ],\n",
    "        \n",
    "        # NOW Foods\n",
    "        \"NOW Foods\": [\n",
    "            \"now foods\", \"now\", \"Ù†Ø§Ùˆ\", \"Ù†Ø§Ùˆ ÙÙˆØ¯Ø²\"\n",
    "        ],\n",
    "        \n",
    "        # Manuka Health\n",
    "        \"Manuka Health\": [\n",
    "            \"manuka health\", \"manuka\", \"Ù…Ø§Ù†ÙˆÚ©Ø§\", \"Ù…Ø§Ù†ÙˆÙƒØ§\"\n",
    "        ],\n",
    "        \n",
    "        # Foods Alive\n",
    "        \"Foods Alive\": [\n",
    "            \"foods alive\", \"alive\"\n",
    "        ],\n",
    "        \n",
    "        # Stevia (if it's a brand, not just ingredient)\n",
    "        \"Stevia\": [\n",
    "            \"stevia\", \"Ø³ØªÛŒÙÛŒØ§\", \"Ø³ØªÙŠÙÙŠØ§\", \"Ø³Ú©Ø± Ø³ØªÛŒÙÛŒØ§\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    return enhanced_mapping\n",
    "\n",
    "def match_brand_with_priority(query: str, brand_mappings: dict) -> tuple:\n",
    "    \"\"\"Match brand with priority rules to avoid misclassification\"\"\"\n",
    "    \n",
    "    query_norm = normalize_text(query)\n",
    "    best_match = None\n",
    "    best_confidence = 0\n",
    "    \n",
    "    # Priority 1: Exact brand name match (highest priority)\n",
    "    for brand, variations in brand_mappings.items():\n",
    "        for variation in variations:\n",
    "            variation_norm = normalize_text(variation)\n",
    "            \n",
    "            # Exact match check\n",
    "            if variation_norm == query_norm:\n",
    "                return brand, 1.0, \"exact_match\"\n",
    "            \n",
    "            # Word boundary exact match\n",
    "            if f\" {variation_norm} \" in f\" {query_norm} \" or \\\n",
    "               query_norm.startswith(f\"{variation_norm} \") or \\\n",
    "               query_norm.endswith(f\" {variation_norm}\"):\n",
    "                confidence = 0.95\n",
    "                if confidence > best_confidence:\n",
    "                    best_match = brand\n",
    "                    best_confidence = confidence\n",
    "    \n",
    "    # Priority 2: Product line disambiguation\n",
    "    # If \"argivit focus\" found, assign to ARGIVIT, not Focus\n",
    "    if \"argivit\" in query_norm and \"focus\" in query_norm:\n",
    "        return \"ARGIVIT\", 0.9, \"product_line_match\"\n",
    "    \n",
    "    # Priority 3: Prevent generic word matches\n",
    "    generic_terms = [\"omega\", \"Ø§ÙˆÙ…ÛŒØ¬Ø§\", \"vitamin\", \"ÙÛŒØªØ§Ù…ÛŒÙ†\"]\n",
    "    for term in generic_terms:\n",
    "        if normalize_text(term) == query_norm:\n",
    "            return \"Other\", 0.0, \"generic_term\"\n",
    "    \n",
    "    # Priority 4: Fuzzy matching with strict thresholds\n",
    "    if best_match and best_confidence >= 0.75:\n",
    "        return best_match, best_confidence, \"fuzzy_match\"\n",
    "    \n",
    "    return \"Other\", 0.0, \"no_match\"\n",
    "\n",
    "def load_brands_from_file(brands_file: str) -> Dict[str, str]:\n",
    "    try:\n",
    "        brands_df = pd.read_excel(brands_file, sheet_name='Brands')\n",
    "        \n",
    "        # Start with enhanced mapping\n",
    "        enhanced_mapping = create_enhanced_brand_mapping()\n",
    "        brand_mapping = {}\n",
    "        \n",
    "        # Add enhanced mappings first\n",
    "        for brand, variations in enhanced_mapping.items():\n",
    "            for variation in variations:\n",
    "                brand_mapping[normalize_text(variation)] = brand\n",
    "                brand_mapping[tight_normalize(variation)] = brand\n",
    "        \n",
    "        # Store original brand names for exact matching\n",
    "        exact_brand_names = {}\n",
    "        \n",
    "        for _, row in brands_df.iterrows():\n",
    "            brand_en = str(row['Brand EN']).strip()\n",
    "            brand_ar = str(row['Brand AR']).strip()\n",
    "            \n",
    "            if brand_en and brand_en != 'nan':\n",
    "                # Store exact matches (case-insensitive but preserve original case)\n",
    "                exact_brand_names[brand_en.lower()] = brand_en\n",
    "                exact_brand_names[tight_normalize(brand_en)] = brand_en\n",
    "                \n",
    "                # Store normalized versions\n",
    "                brand_mapping[brand_en.lower()] = brand_en\n",
    "                brand_mapping[tight_normalize(brand_en)] = brand_en\n",
    "                brand_mapping[normalize_text(brand_en)] = brand_en\n",
    "                \n",
    "            if brand_ar and brand_ar != 'nan':\n",
    "                # Store exact matches for Arabic\n",
    "                exact_brand_names[brand_ar] = brand_en\n",
    "                exact_brand_names[tight_normalize(brand_ar)] = brand_en\n",
    "                exact_brand_names[normalize_text(brand_ar)] = brand_en\n",
    "                \n",
    "                # Store normalized versions\n",
    "                brand_mapping[brand_ar] = brand_en\n",
    "                brand_mapping[tight_normalize(brand_ar)] = brand_en\n",
    "                brand_mapping[normalize_text(brand_ar)] = brand_en\n",
    "        \n",
    "        print(f\"âœ“ Loaded {len(set(brands_df['Brand EN'].dropna()))} brands from file\")\n",
    "        print(f\"âœ“ Enhanced with {len(enhanced_mapping)} predefined brand mappings\")\n",
    "        return brand_mapping, exact_brand_names\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load brands file ({e}), using enhanced mapping only\")\n",
    "        enhanced_mapping = create_enhanced_brand_mapping()\n",
    "        brand_mapping = {}\n",
    "        exact_brand_names = {}\n",
    "        \n",
    "        for brand, variations in enhanced_mapping.items():\n",
    "            for variation in variations:\n",
    "                brand_mapping[normalize_text(variation)] = brand\n",
    "                brand_mapping[tight_normalize(variation)] = brand\n",
    "                exact_brand_names[variation] = brand\n",
    "                \n",
    "        return brand_mapping, exact_brand_names\n",
    "\n",
    "def load_descriptions_from_file(brands_file: str) -> List[str]:\n",
    "    try:\n",
    "        desc_df = pd.read_excel(brands_file, sheet_name='Description')\n",
    "        descriptions = desc_df.iloc[:, 0].dropna().astype(str).tolist()\n",
    "        print(f\"âœ“ Loaded {len(descriptions)} product descriptions\")\n",
    "        return descriptions\n",
    "    except Exception as e:\n",
    "        print(f\"Note: Could not load descriptions ({e})\")\n",
    "        return []\n",
    "\n",
    "def build_brand_word_index(brand_mapping: dict) -> dict:\n",
    "    \"\"\"Build index of brand words to avoid partial matches\"\"\"\n",
    "    brand_words = set()\n",
    "    for brand_key, brand_name in brand_mapping.items():\n",
    "        # Add full brand name words\n",
    "        words = normalize_text(brand_name).split()\n",
    "        for word in words:\n",
    "            if len(word) >= CONFIG[\"min_brand_length\"]:\n",
    "                brand_words.add(word)\n",
    "    return brand_words\n",
    "\n",
    "def build_nutraceutical_brand_catalog(brands_file: str):\n",
    "    file_brands, exact_brand_names = load_brands_from_file(brands_file)\n",
    "    descriptions = load_descriptions_from_file(brands_file)\n",
    "    \n",
    "    # Get canonical brands from enhanced mapping\n",
    "    enhanced_mapping = create_enhanced_brand_mapping()\n",
    "    canonical_brands = list(enhanced_mapping.keys())\n",
    "    \n",
    "    # Add brands from file\n",
    "    canonical_brands.extend(list(set(file_brands.values())))\n",
    "    canonical_brands = list(set(canonical_brands))  # Remove duplicates\n",
    "    canonical_brands.append(\"Other\")\n",
    "    \n",
    "    # Build exact match patterns\n",
    "    exact_matches = {}\n",
    "    word_to_brand = {}  # Map individual words to their brands\n",
    "    regex_patterns = []\n",
    "    \n",
    "    # Process enhanced mappings first\n",
    "    for brand_name, variations in enhanced_mapping.items():\n",
    "        for variation in variations:\n",
    "            tight = tight_normalize(variation)\n",
    "            if tight and len(tight) >= CONFIG[\"min_brand_length\"]:\n",
    "                exact_matches[tight] = brand_name\n",
    "                \n",
    "            # Create regex patterns for exact word boundary matching\n",
    "            if len(variation) >= CONFIG[\"min_brand_length\"]:\n",
    "                escaped = re.escape(variation)\n",
    "                pattern = re.compile(rf\"\\b{escaped}\\b\", re.IGNORECASE | re.UNICODE)\n",
    "                regex_patterns.append((pattern, brand_name))\n",
    "                \n",
    "            # Map individual words to brands (for multi-word brands)\n",
    "            words = normalize_text(variation).split()\n",
    "            if len(words) == 1 and len(words[0]) >= CONFIG[\"min_brand_length\"]:\n",
    "                word_to_brand[words[0]] = brand_name\n",
    "    \n",
    "    # Process file brands\n",
    "    for brand_key, brand_name in file_brands.items():\n",
    "        tight = tight_normalize(brand_key)\n",
    "        if tight and len(tight) >= CONFIG[\"min_brand_length\"]:\n",
    "            exact_matches[tight] = brand_name\n",
    "            \n",
    "        # Create regex patterns for exact word boundary matching\n",
    "        if len(brand_key) >= CONFIG[\"min_brand_length\"]:\n",
    "            escaped = re.escape(brand_key)\n",
    "            pattern = re.compile(rf\"\\b{escaped}\\b\", re.IGNORECASE | re.UNICODE)\n",
    "            regex_patterns.append((pattern, brand_name))\n",
    "            \n",
    "        # Map individual words to brands (for multi-word brands)\n",
    "        words = normalize_text(brand_key).split()\n",
    "        if len(words) == 1 and len(words[0]) >= CONFIG[\"min_brand_length\"]:\n",
    "            word_to_brand[words[0]] = brand_name\n",
    "    \n",
    "    # Build brand word index to avoid false positives\n",
    "    brand_words = build_brand_word_index(file_brands)\n",
    "    \n",
    "    catalog = {\n",
    "        \"canonical\": canonical_brands,\n",
    "        \"exact_matches\": exact_matches,\n",
    "        \"exact_brand_names\": exact_brand_names,\n",
    "        \"word_to_brand\": word_to_brand,\n",
    "        \"regex_patterns\": regex_patterns,\n",
    "        \"file_brands\": file_brands,\n",
    "        \"brand_words\": brand_words,\n",
    "        \"enhanced_mapping\": enhanced_mapping\n",
    "    }\n",
    "    return catalog\n",
    "\n",
    "# ---------------- ENHANCED BRAND MATCHING FUNCTIONS ----------------\n",
    "\n",
    "def is_exact_brand_match(query: str, brand_name: str) -> bool:\n",
    "    \"\"\"Check if query contains exact brand name\"\"\"\n",
    "    query_norm = normalize_text(query)\n",
    "    brand_norm = normalize_text(brand_name)\n",
    "    \n",
    "    # Check if brand name appears as complete word(s) in query\n",
    "    brand_words = brand_norm.split()\n",
    "    query_words = query_norm.split()\n",
    "    \n",
    "    if len(brand_words) == 1:\n",
    "        # Single word brand - must appear as complete word\n",
    "        return brand_words[0] in query_words\n",
    "    else:\n",
    "        # Multi-word brand - check if all words appear consecutively\n",
    "        brand_text = \" \".join(brand_words)\n",
    "        return brand_text in query_norm\n",
    "\n",
    "def calculate_enhanced_confidence(query: str, matched_brand: str, match_type: str, score: float = 0) -> float:\n",
    "    \"\"\"Enhanced confidence calculation with stricter rules\"\"\"\n",
    "    base_confidence = 0.0\n",
    "    \n",
    "    if match_type == \"exact_full\":\n",
    "        base_confidence = 1.0\n",
    "    elif match_type == \"exact_word\":\n",
    "        base_confidence = 0.95\n",
    "    elif match_type == \"regex_exact\":\n",
    "        base_confidence = 0.9\n",
    "    elif match_type == \"product_line_match\":\n",
    "        base_confidence = 0.9\n",
    "    elif match_type == \"corrected_match\":\n",
    "        base_confidence = 0.95\n",
    "    elif match_type == \"arabic_match\":\n",
    "        base_confidence = 0.95\n",
    "    elif match_type == \"fuzzy_very_high\":\n",
    "        base_confidence = min(0.85, score / 100)\n",
    "    elif match_type == \"fuzzy_high\":\n",
    "        base_confidence = min(0.75, score / 100)\n",
    "    else:\n",
    "        base_confidence = 0.5\n",
    "    \n",
    "    # Apply penalties for partial matches\n",
    "    query_words = set(normalize_text(query).split())\n",
    "    brand_words = set(normalize_text(matched_brand).split())\n",
    "    \n",
    "    # If query contains many non-brand words, reduce confidence\n",
    "    if len(query_words) > len(brand_words) * 2:\n",
    "        base_confidence *= 0.8\n",
    "    \n",
    "    # If brand name is much shorter than query, it might be coincidental\n",
    "    brand_length = len(normalize_text(matched_brand).replace(\" \", \"\"))\n",
    "    query_length = len(normalize_text(query).replace(\" \", \"\"))\n",
    "    \n",
    "    if brand_length < query_length * 0.3:  # Brand is less than 30% of query length\n",
    "        base_confidence *= CONFIG[\"partial_match_penalty\"]\n",
    "    \n",
    "    return base_confidence\n",
    "\n",
    "def enhanced_brand_match(query: str, catalog: dict) -> tuple:\n",
    "    \"\"\"Enhanced brand matching with stricter rules and special fixes\"\"\"\n",
    "    if not isinstance(query, str) or not query.strip():\n",
    "        return \"Other\", 0.0\n",
    "    \n",
    "    original_query = query.strip()\n",
    "    norm_query = normalize_text(original_query)\n",
    "    tight_query = tight_normalize(original_query)\n",
    "    query_words = norm_query.split()\n",
    "    query_lower = original_query.lower()\n",
    "    \n",
    "    matches = []\n",
    "    \n",
    "    # SPECIAL CASE FIXES FIRST (highest priority)\n",
    "    \n",
    "    # Fix: ARGIVIT Focus should be ARGIVIT\n",
    "    if any(term in query_lower for term in ['argivit focus', 'Ø§Ø±Ø¬ÙÛŒØª', 'Ø§Ø±Ø¬ÛŒÙØª']):\n",
    "        confidence = calculate_enhanced_confidence(original_query, \"ARGIVIT\", \"corrected_match\")\n",
    "        return \"ARGIVIT\", confidence\n",
    "    \n",
    "    # Fix: Generic Omega-3 should be Other\n",
    "    if any(term in query_lower for term in ['Ø§ÙˆÙ…ÛŒØ¬Ø§ 3', 'Ø§ÙˆÙ…ÛŒØ¬Ø§3']) and 'jamjoom' not in query_lower:\n",
    "        return \"Other\", 0.0\n",
    "    \n",
    "    # Fix: Arabic VITABIOTICS\n",
    "    if 'ÙÛŒØªØ§Ø¨ÛŒÙˆØªÚ©Ø³' in original_query:\n",
    "        confidence = calculate_enhanced_confidence(original_query, \"VITABIOTICS\", \"arabic_match\")\n",
    "        return \"VITABIOTICS\", confidence\n",
    "    \n",
    "    # Check enhanced mappings first\n",
    "    enhanced_mapping = catalog.get(\"enhanced_mapping\", {})\n",
    "    for brand, variations in enhanced_mapping.items():\n",
    "        for variation in variations:\n",
    "            variation_norm = normalize_text(variation)\n",
    "            \n",
    "            # Exact match check\n",
    "            if variation_norm == norm_query:\n",
    "                confidence = calculate_enhanced_confidence(original_query, brand, \"exact_full\")\n",
    "                matches.append((brand, confidence, \"exact_full\"))\n",
    "                \n",
    "            # Word boundary exact match\n",
    "            elif f\" {variation_norm} \" in f\" {norm_query} \" or \\\n",
    "                 norm_query.startswith(f\"{variation_norm} \") or \\\n",
    "                 norm_query.endswith(f\" {variation_norm}\"):\n",
    "                confidence = calculate_enhanced_confidence(original_query, brand, \"exact_word\")\n",
    "                matches.append((brand, confidence, \"exact_word\"))\n",
    "    \n",
    "    # 1. EXACT TIGHT NORMALIZATION MATCH\n",
    "    if tight_query in catalog[\"exact_matches\"]:\n",
    "        brand = catalog[\"exact_matches\"][tight_query]\n",
    "        confidence = calculate_enhanced_confidence(original_query, brand, \"exact_full\")\n",
    "        matches.append((brand, confidence, \"exact_full\"))\n",
    "    \n",
    "    # 2. EXACT BRAND NAME MATCH (check if brand appears exactly in query)\n",
    "    for brand_key, brand_name in catalog[\"exact_brand_names\"].items():\n",
    "        if is_exact_brand_match(original_query, brand_name):\n",
    "            confidence = calculate_enhanced_confidence(original_query, brand_name, \"exact_word\")\n",
    "            matches.append((brand_name, confidence, \"exact_word\"))\n",
    "    \n",
    "    # 3. REGEX PATTERN MATCHING (word boundaries)\n",
    "    for pattern, brand in catalog[\"regex_patterns\"]:\n",
    "        if pattern.search(original_query):\n",
    "            confidence = calculate_enhanced_confidence(original_query, brand, \"regex_exact\")\n",
    "            matches.append((brand, confidence, \"regex_exact\"))\n",
    "    \n",
    "    # 4. SINGLE WORD BRAND MATCHING (only for single-word queries or exact word matches)\n",
    "    if len(query_words) == 1:  # Only for single word queries\n",
    "        single_word = query_words[0]\n",
    "        if single_word in catalog[\"word_to_brand\"]:\n",
    "            brand = catalog[\"word_to_brand\"][single_word]\n",
    "            confidence = calculate_enhanced_confidence(original_query, brand, \"exact_word\")\n",
    "            matches.append((brand, confidence, \"exact_word\"))\n",
    "    else:\n",
    "        # For multi-word queries, check if any single word is an exact brand match\n",
    "        for word in query_words:\n",
    "            if word in catalog[\"word_to_brand\"]:\n",
    "                brand = catalog[\"word_to_brand\"][word]\n",
    "                # Only accept if the word is significant part of the query\n",
    "                if len(word) >= len(norm_query) * 0.4:  # Word is at least 40% of query length\n",
    "                    confidence = calculate_enhanced_confidence(word, brand, \"exact_word\") * 0.8\n",
    "                    matches.append((brand, confidence, \"exact_word\"))\n",
    "    \n",
    "    # 5. FUZZY MATCHING (very strict, only for high scores)\n",
    "    if USE_RAPIDFUZZ and not matches:\n",
    "        try:\n",
    "            # Only match against exact brand names\n",
    "            brand_candidates = list(catalog[\"exact_brand_names\"].values())\n",
    "            brand_candidates.extend([key for key in catalog[\"exact_brand_names\"].keys() if isinstance(key, str)])\n",
    "            \n",
    "            if brand_candidates:\n",
    "                result = process.extractOne(\n",
    "                    norm_query,\n",
    "                    brand_candidates,\n",
    "                    scorer=fuzz.token_sort_ratio,  # More strict than token_set_ratio\n",
    "                    score_cutoff=CONFIG[\"exact_match_threshold\"]  # Very high threshold\n",
    "                )\n",
    "                \n",
    "                if result:\n",
    "                    matched_text, score, _ = result\n",
    "                    # Find the brand name for this match\n",
    "                    brand = catalog[\"exact_brand_names\"].get(matched_text, matched_text)\n",
    "                    if brand in catalog[\"canonical\"]:\n",
    "                        match_type = \"fuzzy_very_high\" if score >= 95 else \"fuzzy_high\"\n",
    "                        confidence = calculate_enhanced_confidence(original_query, brand, match_type, score)\n",
    "                        matches.append((brand, confidence, match_type))\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    # 6. SELECT BEST MATCH\n",
    "    if matches:\n",
    "        # Sort by confidence and match type priority\n",
    "        type_priority = {\n",
    "            \"exact_full\": 0, \"corrected_match\": 1, \"arabic_match\": 2, \"exact_word\": 3, \n",
    "            \"regex_exact\": 4, \"product_line_match\": 5, \"fuzzy_very_high\": 6, \"fuzzy_high\": 7\n",
    "        }\n",
    "        matches.sort(key=lambda x: (-x[1], type_priority.get(x[2], 10)))\n",
    "        \n",
    "        best_brand, best_confidence, best_type = matches[0]\n",
    "        \n",
    "        # Apply minimum confidence threshold\n",
    "        if best_confidence >= CONFIG[\"min_confidence_threshold\"]:\n",
    "            return best_brand, best_confidence\n",
    "    \n",
    "    return \"Other\", 0.0\n",
    "\n",
    "# ---------------- CLUSTERING FUNCTIONS (UNCHANGED) ----------------\n",
    "\n",
    "def cluster_brand_queries(brand_df: pd.DataFrame, start_cluster_id: int) -> pd.DataFrame:\n",
    "    if len(brand_df) <= 1:\n",
    "        brand_df = brand_df.copy()\n",
    "        brand_df[\"Cluster ID\"] = start_cluster_id\n",
    "        return brand_df\n",
    "    queries = brand_df[\"search_clean\"].tolist()\n",
    "    try:\n",
    "        if EMBEDDINGS_OK:\n",
    "            model = SentenceTransformer(CONFIG[\"embedding_model\"])\n",
    "            embeddings = model.encode(queries, batch_size=CONFIG[\"embedding_batch_size\"], \n",
    "                                    show_progress_bar=False, normalize_embeddings=True)\n",
    "            brand_threshold = min(CONFIG[\"distance_threshold\"] * 0.8, 0.4)\n",
    "            clustering = AgglomerativeClustering(\n",
    "                n_clusters=None,\n",
    "                distance_threshold=brand_threshold,\n",
    "                metric='cosine',\n",
    "                linkage='average'\n",
    "            )\n",
    "            labels = clustering.fit_predict(embeddings)\n",
    "        elif USE_RAPIDFUZZ:\n",
    "            n = len(queries)\n",
    "            dist_matrix = np.zeros((n, n))\n",
    "            for i in range(n):\n",
    "                for j in range(i+1, n):\n",
    "                    similarity = fuzz.token_set_ratio(queries[i], queries[j])\n",
    "                    distance = 1 - (similarity / 100.0)\n",
    "                    dist_matrix[i, j] = distance\n",
    "                    dist_matrix[j, i] = distance\n",
    "            clustering = AgglomerativeClustering(\n",
    "                n_clusters=None,\n",
    "                distance_threshold=0.3,\n",
    "                metric='precomputed',\n",
    "                linkage='average'\n",
    "            )\n",
    "            labels = clustering.fit_predict(dist_matrix)\n",
    "        else:\n",
    "            labels = []\n",
    "            current_label = 0\n",
    "            processed = set()\n",
    "            for i, query in enumerate(queries):\n",
    "                if i in processed:\n",
    "                    continue\n",
    "                cluster_members = [i]\n",
    "                processed.add(i)\n",
    "                for j, other_query in enumerate(queries[i+1:], i+1):\n",
    "                    if j in processed:\n",
    "                        continue\n",
    "                    words1 = set(query.split())\n",
    "                    words2 = set(other_query.split())\n",
    "                    if len(words1 & words2) >= min(len(words1), len(words2)) * 0.6:\n",
    "                        cluster_members.append(j)\n",
    "                        processed.add(j)\n",
    "                for idx in cluster_members:\n",
    "                    if idx < len(labels):\n",
    "                        labels[idx] = current_label\n",
    "                    else:\n",
    "                        labels.append(current_label)\n",
    "                current_label += 1\n",
    "            while len(labels) < len(queries):\n",
    "                labels.append(current_label)\n",
    "                current_label += 1\n",
    "        labels = [label + start_cluster_id for label in labels]\n",
    "    except Exception as e:\n",
    "        print(f\"    -> Warning: Brand clustering failed ({e}), using individual clusters\")\n",
    "        labels = list(range(start_cluster_id, start_cluster_id + len(brand_df)))\n",
    "    result_df = brand_df.copy()\n",
    "    result_df[\"Cluster ID\"] = labels\n",
    "    return result_df\n",
    "\n",
    "def cluster_with_embeddings_offset(df: pd.DataFrame, offset: int) -> pd.DataFrame:\n",
    "    queries = df[\"search_clean\"].tolist()\n",
    "    model = SentenceTransformer(CONFIG[\"embedding_model\"])\n",
    "    embeddings = model.encode(queries, batch_size=CONFIG[\"embedding_batch_size\"], \n",
    "                            show_progress_bar=False, normalize_embeddings=True)\n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=None,\n",
    "        distance_threshold=CONFIG[\"distance_threshold\"],\n",
    "        metric='cosine',\n",
    "        linkage='average'\n",
    "    )\n",
    "    labels = clustering.fit_predict(embeddings)\n",
    "    result_df = df.copy()\n",
    "    result_df[\"Cluster ID\"] = labels + offset\n",
    "    return result_df\n",
    "\n",
    "def cluster_with_tfidf_offset(df: pd.DataFrame, offset: int) -> pd.DataFrame:\n",
    "    queries = df[\"search_clean\"].tolist()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=15000)\n",
    "    X = vectorizer.fit_transform(queries)\n",
    "    n_clusters = max(2, min(int(math.sqrt(len(queries))), len(queries)//2))\n",
    "    clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "    labels = clustering.fit_predict(X.toarray())\n",
    "    result_df = df.copy()\n",
    "    result_df[\"Cluster ID\"] = labels + offset\n",
    "    return result_df\n",
    "\n",
    "def cluster_with_fuzzy_offset(df: pd.DataFrame, offset: int) -> pd.DataFrame:\n",
    "    queries = df[\"search_clean\"].tolist()\n",
    "    n = len(queries)\n",
    "    if n <= 1:\n",
    "        result_df = df.copy()\n",
    "        result_df[\"Cluster ID\"] = offset\n",
    "        return result_df\n",
    "    dist_matrix = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if USE_RAPIDFUZZ:\n",
    "                similarity = fuzz.partial_ratio(queries[i], queries[j])\n",
    "            else:\n",
    "                similarity = difflib.SequenceMatcher(None, queries[i], queries[j]).ratio() * 100\n",
    "            distance = 1 - (similarity / 100.0)\n",
    "            dist_matrix[i, j] = distance\n",
    "            dist_matrix[j, i] = distance\n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=None,\n",
    "        distance_threshold=1 - (CONFIG[\"fallback_fuzzy_threshold\"]/100),\n",
    "        metric='precomputed',\n",
    "        linkage='average'\n",
    "    )\n",
    "    labels = clustering.fit_predict(dist_matrix)\n",
    "    result_df = df.copy()\n",
    "    result_df[\"Cluster ID\"] = labels + offset\n",
    "    return result_df\n",
    "\n",
    "def perform_brand_aware_clustering(df: pd.DataFrame) -> Tuple[pd.DataFrame, str]:\n",
    "    if df.empty:\n",
    "        df[\"Cluster ID\"] = []\n",
    "        return df, \"empty\"\n",
    "    if len(df) < CONFIG[\"simple_clustering_threshold\"]:\n",
    "        print(f\"    -> Using simple clustering for small dataset ({len(df)} rows)\")\n",
    "        result_df = df.copy()\n",
    "        result_df[\"Cluster ID\"] = range(len(df))\n",
    "        return result_df, \"simple_identity\"\n",
    "    \n",
    "    branded_df = df[df[\"Brand\"] != \"Other\"].copy()\n",
    "    other_df = df[df[\"Brand\"] == \"Other\"].copy()\n",
    "    results = []\n",
    "    cluster_id_counter = 0\n",
    "    \n",
    "    if not branded_df.empty:\n",
    "        print(f\"    -> Clustering {len(branded_df)} branded queries...\")\n",
    "        for brand, brand_group in branded_df.groupby(\"Brand\"):\n",
    "            if len(brand_group) >= CONFIG[\"brand_cluster_min_size\"]:\n",
    "                brand_clustered = cluster_brand_queries(brand_group, cluster_id_counter)\n",
    "                cluster_id_counter = brand_clustered[\"Cluster ID\"].max() + 1\n",
    "                results.append(brand_clustered)\n",
    "            else:\n",
    "                brand_group = brand_group.copy()\n",
    "                brand_group[\"Cluster ID\"] = range(cluster_id_counter, cluster_id_counter + len(brand_group))\n",
    "                cluster_id_counter += len(brand_group)\n",
    "                results.append(brand_group)\n",
    "    \n",
    "    if not other_df.empty:\n",
    "        print(f\"    -> Clustering {len(other_df)} unclassified queries...\")\n",
    "        try:\n",
    "            if EMBEDDINGS_OK:\n",
    "                other_clustered = cluster_with_embeddings_offset(other_df, cluster_id_counter)\n",
    "                method = \"brand_aware_embeddings\"\n",
    "            elif TfidfVectorizer is not None:\n",
    "                other_clustered = cluster_with_tfidf_offset(other_df, cluster_id_counter)\n",
    "                method = \"brand_aware_tfidf\"\n",
    "            elif USE_RAPIDFUZZ:\n",
    "                other_clustered = cluster_with_fuzzy_offset(other_df, cluster_id_counter)\n",
    "                method = \"brand_aware_fuzzy\"\n",
    "            else:\n",
    "                other_clustered = other_df.copy()\n",
    "                other_clustered[\"Cluster ID\"] = range(cluster_id_counter, cluster_id_counter + len(other_df))\n",
    "                method = \"brand_aware_identity\"\n",
    "            results.append(other_clustered)\n",
    "        except Exception as e:\n",
    "            print(f\"    -> Warning: Other clustering failed ({e}), using identity fallback\")\n",
    "            other_df_copy = other_df.copy()\n",
    "            other_df_copy[\"Cluster ID\"] = range(cluster_id_counter, cluster_id_counter + len(other_df))\n",
    "            results.append(other_df_copy)\n",
    "            method = \"brand_aware_identity_fallback\"\n",
    "    else:\n",
    "        method = \"brand_aware_no_other\"\n",
    "    \n",
    "    if results:\n",
    "        final_df = pd.concat(results, ignore_index=True)\n",
    "    else:\n",
    "        final_df = df.copy()\n",
    "        final_df[\"Cluster ID\"] = range(len(final_df))\n",
    "        method = \"brand_aware_empty_fallback\"\n",
    "    \n",
    "    return final_df, method\n",
    "\n",
    "def process_nutraceutical_data_chunk(df: pd.DataFrame, brand_catalog: dict):\n",
    "    df[\"search_original\"] = df[\"search\"].astype(str)\n",
    "    df[\"search_clean\"] = df[\"search_original\"].apply(enhanced_clustering_normalize)\n",
    "    \n",
    "    print(f\"  -> Mapping brands for {len(df)} queries...\")\n",
    "    brands = []\n",
    "    brand_confidences = []\n",
    "    \n",
    "    for i, query in enumerate(df[\"search_original\"]):\n",
    "        if i % 1000 == 0 and i > 0:\n",
    "            print(f\"    -> Processed {i}/{len(df)} brand mappings...\")\n",
    "        \n",
    "        brand, confidence = enhanced_brand_match(query, brand_catalog)\n",
    "        brands.append(brand)\n",
    "        brand_confidences.append(confidence)\n",
    "    \n",
    "    df[\"Brand\"] = brands\n",
    "    df[\"brand_confidence\"] = brand_confidences\n",
    "    \n",
    "    if CONFIG[\"use_dedup\"]:\n",
    "        agg_cols = [\"search_clean\", \"Brand\"]\n",
    "        base = df.groupby(agg_cols, as_index=False).agg({\n",
    "            \"count\": \"sum\",\n",
    "            \"Clicks\": \"sum\",\n",
    "            \"Conversions\": \"sum\",\n",
    "            \"brand_confidence\": \"mean\",\n",
    "            \"search_original\": \"first\",\n",
    "            \"category\": \"first\",\n",
    "            \"clickThroughRate\": \"mean\",\n",
    "            \"conversionRate\": \"mean\",\n",
    "            \"averageClickPosition\": \"mean\",\n",
    "            \"underperforming\": \"first\",\n",
    "            \"start_date\": \"first\",\n",
    "            \"end_date\": \"first\"\n",
    "        })\n",
    "    else:\n",
    "        base = df.copy()\n",
    "    \n",
    "    clustered, method_used = perform_brand_aware_clustering(base)\n",
    "    cluster_map = clustered[[\"search_clean\", \"Brand\", \"Cluster ID\"]]\n",
    "    df_merged = df.merge(cluster_map, on=[\"search_clean\", \"Brand\"], how=\"left\")\n",
    "    \n",
    "    queries_clustered = df_merged.sort_values(\n",
    "        by=[\"Cluster ID\", \"Brand\", \"count\"],\n",
    "        ascending=[True, True, False]\n",
    "    ).reset_index(drop=True)\n",
    "    \n",
    "    return {\n",
    "        \"method\": method_used,\n",
    "        \"queries_clustered\": queries_clustered,\n",
    "        \"clustered_base\": clustered\n",
    "    }\n",
    "\n",
    "def process_nutraceutical_data(df: pd.DataFrame, brand_catalog: dict):\n",
    "    results = []\n",
    "    for i in range(0, len(df), CONFIG[\"chunk_size\"]):\n",
    "        chunk = df.iloc[i:i+CONFIG[\"chunk_size\"]].copy()\n",
    "        print(f\"  -> Processing chunk {i//CONFIG['chunk_size'] + 1} ({len(chunk)} rows)...\")\n",
    "        chunk_result = process_nutraceutical_data_chunk(chunk, brand_catalog)\n",
    "        results.append(chunk_result[\"queries_clustered\"])\n",
    "    \n",
    "    final_df = pd.concat(results, ignore_index=True)\n",
    "    \n",
    "    return {\n",
    "        \"method\": chunk_result[\"method\"],\n",
    "        \"queries_clustered\": final_df,\n",
    "        \"clustered_base\": chunk_result[\"clustered_base\"]\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    print(\"=== VALIDATING INPUT FILES ===\")\n",
    "    if not os.path.exists(CONFIG[\"input_file\"]):\n",
    "        raise FileNotFoundError(f\"Input file not found: {CONFIG['input_file']}\")\n",
    "    if not os.path.exists(CONFIG[\"brands_file\"]):\n",
    "        raise FileNotFoundError(f\"Brands file not found: {CONFIG['brands_file']}\")\n",
    "    print(\"âœ“ Input files validated successfully\")\n",
    "    \n",
    "    print(\"\\n=== BUILDING NUTRACEUTICALS BRAND CATALOG ===\")\n",
    "    try:\n",
    "        brand_catalog = build_nutraceutical_brand_catalog(CONFIG[\"brands_file\"])\n",
    "        print(f\"âœ“ Loaded {len(brand_catalog['canonical'])} canonical brands\")\n",
    "        print(f\"âœ“ Built {len(brand_catalog['exact_matches'])} exact match patterns\")\n",
    "        print(f\"âœ“ Built {len(brand_catalog['word_to_brand'])} word-to-brand mappings\")\n",
    "        print(f\"âœ“ Built {len(brand_catalog['regex_patterns'])} regex patterns\")\n",
    "        print(f\"âœ“ Enhanced with {len(brand_catalog['enhanced_mapping'])} predefined mappings\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error building brand catalog: {e}\")\n",
    "    \n",
    "    print(f\"\\n=== LOADING NUTRACEUTICALS SEARCH DATA ===\")\n",
    "    try:\n",
    "        df = pd.read_excel(CONFIG[\"input_file\"])\n",
    "        print(f\"âœ“ Loaded {len(df)} search queries\")\n",
    "        \n",
    "        print(f\"\\n=== PROCESSING DATA ===\")\n",
    "        result = process_nutraceutical_data(df, brand_catalog)\n",
    "        print(f\"âœ“ Method: {result['method']}\")\n",
    "        print(f\"âœ“ Created {result['queries_clustered']['Cluster ID'].nunique()} clusters\")\n",
    "        \n",
    "        brand_stats = result[\"queries_clustered\"][\"Brand\"].value_counts()\n",
    "        print(f\"âœ“ Top brands: {dict(brand_stats.head(10))}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error processing data: {e}\")\n",
    "    \n",
    "    final_columns = [\n",
    "        \"category\", \"Brand\", \"search\", \"count\", \"Clicks\", \"Conversions\", \n",
    "        \"clickThroughRate\", \"conversionRate\", \"averageClickPosition\", \n",
    "        \"underperforming\", \"start_date\", \"end_date\", \"Cluster ID\"\n",
    "    ]\n",
    "    \n",
    "    final_df = result[\"queries_clustered\"].copy()\n",
    "    for col in final_columns:\n",
    "        if col not in final_df.columns:\n",
    "            if col == \"Brand\" or col == \"Cluster ID\":\n",
    "                continue\n",
    "            else:\n",
    "                final_df[col] = None\n",
    "    \n",
    "    final_df = final_df[final_columns]\n",
    "    \n",
    "    print(f\"\\n=== SAVING RESULTS ===\")\n",
    "    try:\n",
    "        with pd.ExcelWriter(CONFIG[\"output_file\"], engine=\"xlsxwriter\") as writer:\n",
    "            final_df.to_excel(writer, sheet_name=\"enhanced_data\", index=False)\n",
    "            \n",
    "            workbook = writer.book\n",
    "            worksheet = writer.sheets[\"enhanced_data\"]\n",
    "            \n",
    "            header_format = workbook.add_format({\n",
    "                'bold': True,\n",
    "                'text_wrap': True,\n",
    "                'valign': 'top',\n",
    "                'fg_color': '#D7E4BC',\n",
    "                'border': 1\n",
    "            })\n",
    "            \n",
    "            number_format = workbook.add_format({'num_format': '#,##0'})\n",
    "            percent_format = workbook.add_format({'num_format': '0.00%'})\n",
    "            \n",
    "            for i, col in enumerate(final_df.columns):\n",
    "                max_length = len(str(col))\n",
    "                if len(final_df) > 0:\n",
    "                    max_length = max(max_length, final_df[col].astype(str).str.len().max())\n",
    "                max_length = min(max_length, 50)\n",
    "                max_length = max(max_length, 10)\n",
    "                worksheet.set_column(i, i, max_length)\n",
    "                \n",
    "                if col in ['count', 'Clicks', 'Conversions', 'Cluster ID']:\n",
    "                    worksheet.set_column(i, i, None, number_format)\n",
    "                elif col in ['clickThroughRate', 'conversionRate']:\n",
    "                    worksheet.set_column(i, i, None, percent_format)\n",
    "            \n",
    "            worksheet.set_row(0, None, header_format)\n",
    "        \n",
    "        print(f\"âœ“ Results saved to: {CONFIG['output_file']} (enhanced_data sheet only)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving results with formatting: {e}\")\n",
    "        try:\n",
    "            with pd.ExcelWriter(CONFIG[\"output_file\"], engine=\"openpyxl\") as writer:\n",
    "                final_df.to_excel(writer, sheet_name=\"enhanced_data\", index=False)\n",
    "            print(f\"âœ“ Results saved to: {CONFIG['output_file']} (basic format)\")\n",
    "        except Exception as e2:\n",
    "            raise ValueError(f\"Failed to save results: {e2}\")\n",
    "    \n",
    "    print(f\"\\n=== FINAL STATISTICS ===\")\n",
    "    print(f\"ðŸ“Š Total queries processed: {len(final_df):,}\")\n",
    "    print(f\"ðŸ“Š Total clusters created: {final_df['Cluster ID'].nunique():,}\")\n",
    "    print(f\"ðŸ“Š Brands identified: {final_df['Brand'].nunique()}\")\n",
    "    \n",
    "    brand_counts = final_df[\"Brand\"].value_counts()\n",
    "    total_queries = len(final_df)\n",
    "    \n",
    "    print(f\"\\n=== BRAND MAPPING RESULTS ===\")\n",
    "    print(f\"ðŸ·ï¸  Branded queries: {len(final_df[final_df['Brand'] != 'Other']):,} ({len(final_df[final_df['Brand'] != 'Other'])/total_queries*100:.1f}%)\")\n",
    "    print(f\"â“ Unclassified (Other): {len(final_df[final_df['Brand'] == 'Other']):,} ({len(final_df[final_df['Brand'] == 'Other'])/total_queries*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nðŸ† Top 15 brands by query count:\")\n",
    "    for i, (brand, count) in enumerate(brand_counts.head(15).items(), 1):\n",
    "        percentage = count / total_queries * 100\n",
    "        total_searches = final_df[final_df[\"Brand\"] == brand][\"count\"].sum()\n",
    "        print(f\"  {i:2d}. {brand:<20}: {count:>4,} queries ({percentage:>5.1f}%) | {total_searches:>8,} searches\")\n",
    "    \n",
    "    total_search_volume = final_df[\"count\"].sum()\n",
    "    print(f\"\\n=== SEARCH VOLUME STATISTICS ===\")\n",
    "    print(f\"ðŸ“ˆ Total search volume: {total_search_volume:,}\")\n",
    "    print(f\"ðŸ“ˆ Average searches per query: {total_search_volume/total_queries:.1f}\")\n",
    "    \n",
    "    print(f\"\\nðŸ” Top 10 queries by search volume:\")\n",
    "    top_queries = final_df.nlargest(10, \"count\")\n",
    "    for i, (_, row) in enumerate(top_queries.iterrows(), 1):\n",
    "        print(f\"  {i:2d}. {row['search']:<30} [{row['Brand']:<15}]: {row['count']:>6,} searches\")\n",
    "    \n",
    "    print(f\"\\n=== CLUSTER ANALYSIS ===\")\n",
    "    cluster_sizes = final_df.groupby('Cluster ID').size()\n",
    "    print(f\"ðŸ“Š Average cluster size: {cluster_sizes.mean():.1f} queries\")\n",
    "    print(f\"ðŸ“Š Largest cluster: {cluster_sizes.max()} queries\")\n",
    "    print(f\"ðŸ“Š Smallest cluster: {cluster_sizes.min()} queries\")\n",
    "    \n",
    "    brand_cluster_counts = final_df.groupby('Brand')['Cluster ID'].nunique().sort_values(ascending=False)\n",
    "    print(f\"\\nðŸ“Š Top brands by cluster count:\")\n",
    "    for brand, cluster_count in brand_cluster_counts.head(10).items():\n",
    "        query_count = len(final_df[final_df['Brand'] == brand])\n",
    "        avg_cluster_size = query_count / cluster_count if cluster_count > 0 else 0\n",
    "        print(f\"  {brand:<20}: {cluster_count:>3} clusters | {query_count:>4} queries | avg size: {avg_cluster_size:.1f}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ‰ Processing completed successfully!\")\n",
    "    print(f\"ðŸ“ Output file: {CONFIG['output_file']}\")\n",
    "    print(f\"     - enhanced_data: Main data with Brand column and Cluster ID\")\n",
    "    print(f\"\\nðŸ“‹ Column sequence in enhanced_data:\")\n",
    "    print(f\"     {' | '.join(final_columns)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f04f6d-b673-4666-b1d8-e4be22583b6b",
   "metadata": {},
   "source": [
    "## STEP 2: Rearrange (Cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4305a79-569a-49f8-8a04-c14fd947daad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 16:05:24,506 - INFO - rapidfuzz available for enhanced fallback matching.\n",
      "2025-09-30 16:05:24,511 - INFO - Loading multilingual sentence transformer model...\n",
      "2025-09-30 16:05:24,513 - ERROR - Failed to load SentenceTransformer: SentenceTransformer.__init__() got an unexpected keyword argument 'verify_ssl'\n",
      "2025-09-30 16:05:24,515 - INFO - Falling back to rapidfuzz matching.\n",
      "2025-09-30 16:05:24,781 - INFO - Processing all sheets: ['queries_clustered', 'generic_type']\n",
      "2025-09-30 16:05:24,813 - INFO - Reading sheet: queries_clustered\n",
      "2025-09-30 16:05:30,202 - INFO - No embedding model. Using rapidfuzz matching.\n",
      "2025-09-30 16:05:30,224 - INFO - Processing 15243 search terms with fuzzy matching...\n",
      "2025-09-30 16:14:15,314 - INFO - Fuzzy matching clustering complete: 690 clusters (threshold: 70).\n",
      "2025-09-30 16:14:24,622 - INFO - Processed sheet: queries_clustered (rows: 15243, clusters: 690)\n",
      "2025-09-30 16:14:24,624 - INFO - Reading sheet: generic_type\n",
      "2025-09-30 16:14:28,759 - INFO - No embedding model. Using rapidfuzz matching.\n",
      "2025-09-30 16:14:28,765 - INFO - Processing 12784 search terms with fuzzy matching...\n",
      "2025-09-30 16:20:34,787 - INFO - Fuzzy matching clustering complete: 592 clusters (threshold: 70).\n",
      "2025-09-30 16:20:38,359 - INFO - Processed sheet: generic_type (rows: 12784, clusters: 592)\n",
      "2025-09-30 16:20:50,861 - INFO - \n",
      "Output saved to: NUTRACEUTICALS_Rearranged_Clusters.xlsx\n",
      "2025-09-30 16:20:50,864 - INFO - Rearrangement complete. Sorted by cluster_id (asc) and count (desc).\n",
      "2025-09-30 16:20:50,865 - INFO - Example: 'Ø§ÙˆÙ…ÛŒØ¬Ø§ Ù£', 'Ø§ÙˆÙ…ÛŒØºØ§ Ù£', 'omega3' in same cluster.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Rearrange Nutraceuticals Search Queries by Word Similarity\n",
    "-------------------------------------------------------\n",
    "Purpose:\n",
    "  - Load \"NUTRACEUTICALS_Enhanced_with_Brands_and_Clusters.xlsx\"\n",
    "  - Process all sheets\n",
    "  - Cluster similar search terms using multilingual embeddings (SentenceTransformer â†’ Agglomerative)\n",
    "  - Fallback to rapidfuzz partial_ratio for better grouping (e.g., \"Ø§ÙˆÙ…ÛŒØ¬Ø§ Ù£\", \"Ø§ÙˆÙ…ÛŒØºØ§ Ù£\")\n",
    "  - Sort by cluster_id (asc) and count (desc)\n",
    "  - Add only cluster_id column, preserve all other columns unchanged\n",
    "  - Handle Arabic/English variations (e.g., \"Ù…ÛŒÙ„Ø§ØªÙˆÙ†ÛŒÙ†\", \"melatonin\")\n",
    "\n",
    "Enhancements:\n",
    "  - DISTANCE_THRESHOLD=0.5 for balanced clusters (fewer, meaningful groups)\n",
    "  - Uses rapidfuzz.partial_ratio (threshold=70) for fuzzy fallback\n",
    "  - No normalization or aggregation to preserve original terms and avoid errors\n",
    "  - Robust error handling and logging\n",
    "\n",
    "Requirements:\n",
    "  pip install pandas numpy scikit-learn openpyxl sentence-transformers rapidfuzz\n",
    "\n",
    "Usage:\n",
    "  python rearrange_nutraceuticals.py\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import logging\n",
    "import os\n",
    "import ssl\n",
    "import urllib3\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check if rapidfuzz is available\n",
    "FUZZY_AVAILABLE = False\n",
    "try:\n",
    "    from rapidfuzz import fuzz\n",
    "    FUZZY_AVAILABLE = True\n",
    "    logger.info(\"rapidfuzz available for enhanced fallback matching.\")\n",
    "except ModuleNotFoundError:\n",
    "    logger.warning(\"rapidfuzz not installed. Install with: pip install rapidfuzz\")\n",
    "\n",
    "# Define file paths\n",
    "INPUT_FILE = 'NUTRACEUTICALS_Enhanced_with_Brands_and_Clusters.xlsx'\n",
    "OUTPUT_FILE = 'NUTRACEUTICALS_Rearranged_Clusters.xlsx'\n",
    "CACHE_FOLDER = './model_cache'\n",
    "\n",
    "# Clustering parameters\n",
    "DISTANCE_THRESHOLD = 0.5  # Higher for fewer, larger clusters\n",
    "FUZZY_THRESHOLD = 70      # Lower for more grouping in fuzzy fallback\n",
    "BATCH_SIZE = 32\n",
    "SEARCH_COL = 'search'\n",
    "SORT_COL = 'count'\n",
    "\n",
    "# SSL bypass option (use cautiously)\n",
    "BYPASS_SSL = False\n",
    "if BYPASS_SSL:\n",
    "    logger.warning(\"SSL verification disabled. Use only for testing.\")\n",
    "    ssl._create_default_https_context = ssl._create_unverified_context\n",
    "    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "# Load the multilingual sentence transformer model\n",
    "model = None\n",
    "try:\n",
    "    logger.info(\"Loading multilingual sentence transformer model...\")\n",
    "    model = SentenceTransformer(\n",
    "        'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
    "        cache_folder=CACHE_FOLDER,\n",
    "        use_auth_token=False,\n",
    "        verify_ssl=not BYPASS_SSL\n",
    "    )\n",
    "    logger.info(\"Model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load SentenceTransformer: {str(e)}\")\n",
    "    if FUZZY_AVAILABLE:\n",
    "        logger.info(\"Falling back to rapidfuzz matching.\")\n",
    "    else:\n",
    "        logger.warning(\"rapidfuzz not installed. No clustering if embeddings fail.\")\n",
    "\n",
    "# Process sheet with embeddings\n",
    "def process_sheet_embeddings(df, search_col='search', sort_col='count', distance_threshold=0.5):\n",
    "    if search_col not in df.columns:\n",
    "        logger.warning(f\"Column '{search_col}' not found. Skipping rearrangement.\")\n",
    "        return df\n",
    "    \n",
    "    searches = df[search_col].fillna('').astype(str).tolist()\n",
    "    num_searches = len(searches)\n",
    "    logger.info(f\"Processing {num_searches} search terms (original text preserved)...\")\n",
    "    \n",
    "    embeddings = model.encode(searches, show_progress_bar=True, batch_size=BATCH_SIZE)\n",
    "    similarity_matrix = cosine_similarity(embeddings)\n",
    "    distance_matrix = 1 - similarity_matrix\n",
    "    \n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=None,\n",
    "        metric='precomputed',\n",
    "        linkage='average',\n",
    "        distance_threshold=distance_threshold\n",
    "    )\n",
    "    labels = clustering.fit_predict(distance_matrix)\n",
    "    \n",
    "    empty_mask = np.array(searches) == ''\n",
    "    labels[empty_mask] = -1\n",
    "    \n",
    "    df['cluster_id'] = labels\n",
    "    \n",
    "    if sort_col in df.columns:\n",
    "        df[sort_col] = pd.to_numeric(df[sort_col], errors='coerce').fillna(0)\n",
    "    else:\n",
    "        logger.warning(f\"Column '{sort_col}' not found. Sorting by cluster_id only.\")\n",
    "        sort_col = None\n",
    "    \n",
    "    sort_cols = ['cluster_id']\n",
    "    if sort_col:\n",
    "        sort_cols.append(sort_col)\n",
    "    df_sorted = df.sort_values(by=sort_cols, ascending=[True, False] if sort_col else [True])\n",
    "    \n",
    "    logger.info(f\"Embedding-based clustering complete: {len(np.unique(labels))} clusters (threshold: {distance_threshold}).\")\n",
    "    return df_sorted\n",
    "\n",
    "# Fallback: Fuzzy matching\n",
    "def process_sheet_fuzzy(df, search_col='search', sort_col='count', threshold=70):\n",
    "    if search_col not in df.columns:\n",
    "        logger.warning(f\"Column '{search_col}' not found. Skipping rearrangement.\")\n",
    "        return df\n",
    "    \n",
    "    searches = df[search_col].fillna('').astype(str).tolist()\n",
    "    num_searches = len(searches)\n",
    "    logger.info(f\"Processing {num_searches} search terms with fuzzy matching...\")\n",
    "    \n",
    "    dist_matrix = np.zeros((num_searches, num_searches))\n",
    "    for i in range(num_searches):\n",
    "        for j in range(i + 1, num_searches):\n",
    "            sim = fuzz.partial_ratio(searches[i], searches[j])\n",
    "            dist = 100 - sim\n",
    "            dist_matrix[i, j] = dist / 100.0\n",
    "            dist_matrix[j, i] = dist / 100.0\n",
    "    \n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=None,\n",
    "        metric='precomputed',\n",
    "        linkage='average',\n",
    "        distance_threshold=(100 - threshold) / 100.0\n",
    "    )\n",
    "    labels = clustering.fit_predict(dist_matrix)\n",
    "    \n",
    "    empty_mask = np.array(searches) == ''\n",
    "    labels[empty_mask] = -1\n",
    "    \n",
    "    df['cluster_id'] = labels\n",
    "    \n",
    "    if sort_col in df.columns:\n",
    "        df[sort_col] = pd.to_numeric(df[sort_col], errors='coerce').fillna(0)\n",
    "    else:\n",
    "        logger.warning(f\"Column '{sort_col}' not found. Sorting by cluster_id only.\")\n",
    "        sort_col = None\n",
    "    \n",
    "    sort_cols = ['cluster_id']\n",
    "    if sort_col:\n",
    "        sort_cols.append(sort_col)\n",
    "    df_sorted = df.sort_values(by=sort_cols, ascending=[True, False] if sort_col else [True])\n",
    "    \n",
    "    logger.info(f\"Fuzzy matching clustering complete: {len(np.unique(labels))} clusters (threshold: {threshold}).\")\n",
    "    return df_sorted\n",
    "\n",
    "# Main processing function\n",
    "def process_sheet(df, search_col='search', sort_col='count', distance_threshold=0.5):\n",
    "    if model:\n",
    "        try:\n",
    "            return process_sheet_embeddings(df, search_col, sort_col, distance_threshold)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Embedding-based clustering failed: {str(e)}.\")\n",
    "            if FUZZY_AVAILABLE:\n",
    "                logger.info(\"Falling back to rapidfuzz matching.\")\n",
    "                return process_sheet_fuzzy(df, search_col, sort_col, threshold=FUZZY_THRESHOLD)\n",
    "            else:\n",
    "                logger.warning(\"rapidfuzz not installed. Returning original DataFrame.\")\n",
    "                return df\n",
    "    else:\n",
    "        if FUZZY_AVAILABLE:\n",
    "            logger.info(\"No embedding model. Using rapidfuzz matching.\")\n",
    "            return process_sheet_fuzzy(df, search_col, sort_col, threshold=FUZZY_THRESHOLD)\n",
    "        else:\n",
    "            logger.warning(\"No clustering possible. Returning original DataFrame.\")\n",
    "            return df\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    try:\n",
    "        xls = pd.ExcelFile(INPUT_FILE)\n",
    "        sheet_names = xls.sheet_names\n",
    "        logger.info(f\"Processing all sheets: {sheet_names}\")\n",
    "        \n",
    "        with pd.ExcelWriter(OUTPUT_FILE, engine='openpyxl') as writer:\n",
    "            for sheet_name in sheet_names:\n",
    "                logger.info(f\"Reading sheet: {sheet_name}\")\n",
    "                df = pd.read_excel(INPUT_FILE, sheet_name=sheet_name)\n",
    "                \n",
    "                df_rearranged = process_sheet(df, search_col=SEARCH_COL, sort_col=SORT_COL, distance_threshold=DISTANCE_THRESHOLD)\n",
    "                df_rearranged.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "                \n",
    "                logger.info(f\"Processed sheet: {sheet_name} (rows: {len(df_rearranged)}, clusters: {df_rearranged['cluster_id'].nunique()})\")\n",
    "        \n",
    "        logger.info(f\"\\nOutput saved to: {OUTPUT_FILE}\")\n",
    "        logger.info(\"Rearrangement complete. Sorted by cluster_id (asc) and count (desc).\")\n",
    "        logger.info(\"Example: 'Ø§ÙˆÙ…ÛŒØ¬Ø§ Ù£', 'Ø§ÙˆÙ…ÛŒØºØ§ Ù£', 'omega3' in same cluster.\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Input file '{INPUT_FILE}' not found.\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1cd895-8f2d-4549-aa83-b1bb5d99046d",
   "metadata": {},
   "source": [
    "## STEP 3: Add Category Split + 3-Month Aggregated CTR / CR (In-Place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28105e40-2381-4b70-9062-cec6d365ab02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Backup created: NUTRACEUTICALS_Rearranged_Clusters_backup_step3.xlsx\n",
      "[INFO] Loaded 15243 rows from sheet 'queries_clustered'\n",
      "[INFO] Splitting category column into components...\n",
      "[INFO] Aggregating by: ['search', 'Department', 'Category', 'Sub Category']\n",
      "[INFO] Created 5253 aggregate groups\n",
      "[DONE] Sheet 'queries_clustered' updated in workbook: NUTRACEUTICALS_Rearranged_Clusters.xlsx\n",
      "[INFO] Final dataset has 15243 rows and 24 columns\n",
      "\n",
      "[INFO] Sample of new aggregate columns:\n",
      " search  total_impressions_3m  total_clicks_3m  ctr_3m    cr_3m\n",
      "hemadid                  1795            352.0  0.1961 0.103621\n",
      "hemadid                  1795            352.0  0.1961 0.103621\n",
      "hemadid                  1795            352.0  0.1961 0.103621\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "STEP 3: Add Category Split + 3-Month Aggregated CTR / CR (In-Place)\n",
    "------------------------------------------------------------------\n",
    "Updates ONLY the sheet 'queries_clustered' in:\n",
    "    NUTRACEUTICALS_Enhanced_with_Brands_and_Clusters.xlsx\n",
    "\n",
    "Actions:\n",
    "  - Split 'category' into Department / Category / Sub Category / Class / Sub Class (delimiter: ///)\n",
    "  - Aggregate 3-month totals per GROUP_BY_COLUMNS\n",
    "  - Add columns:\n",
    "      total_impressions_3m\n",
    "      total_clicks_3m  \n",
    "      total_conversions_3m\n",
    "      ctr_3m\n",
    "      cr_3m\n",
    "  - Remove any pre-existing aggregate columns before recalculation\n",
    "  - DO NOT touch other sheets\n",
    "  - Creates a single backup once\n",
    "\"\"\"\n",
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "FILE_PATH = \"NUTRACEUTICALS_Rearranged_Clusters.xlsx\"\n",
    "TARGET_SHEET = \"queries_clustered\"  # Change if your sheet has different name\n",
    "BACKUP_BEFORE_OVERWRITE = True\n",
    "GROUP_BY_COLUMNS = [\"search\",\"Department\",\"Category\",\"Sub Category\"]  # Updated as requested\n",
    "\n",
    "REQUIRED_CORE = [\n",
    "    \"category\",\"search\",\"count\",\"Clicks\",\"Conversions\",\n",
    "    \"start_date\",\"end_date\"\n",
    "]\n",
    "\n",
    "NUMERIC_COLS = [\"count\",\"Clicks\",\"Conversions\"]\n",
    "\n",
    "AGG_COLS = [\n",
    "    \"total_impressions_3m\",\n",
    "    \"total_clicks_3m\",\n",
    "    \"total_conversions_3m\",\n",
    "    \"ctr_3m\",\n",
    "    \"cr_3m\"\n",
    "]\n",
    "\n",
    "def ensure_core(df: pd.DataFrame):\n",
    "    \"\"\"Check if all required columns exist\"\"\"\n",
    "    missing = [c for c in REQUIRED_CORE if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required core columns: {missing}\")\n",
    "\n",
    "def split_category_path(val):\n",
    "    \"\"\"\n",
    "    Split category path with /// delimiter\n",
    "    Returns: (Department, Category, Sub Category, Class, Sub Class)\n",
    "    \"\"\"\n",
    "    if not isinstance(val, str):\n",
    "        return (None, None, None, None, None)\n",
    "    \n",
    "    val = val.strip()\n",
    "    parts = [p.strip() for p in val.split(\"///\")]\n",
    "    parts = [p for p in parts if p]  # Remove empty parts\n",
    "    \n",
    "    # Pad with None to ensure we have 5 elements\n",
    "    while len(parts) < 5:\n",
    "        parts.append(None)\n",
    "    \n",
    "    return tuple(parts[:5])  # Return first 5 elements\n",
    "\n",
    "def add_category_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Add Department, Category, Sub Category, Class, Sub Class columns\"\"\"\n",
    "    # Check if columns already exist\n",
    "    new_cols = [\"Department\",\"Category\",\"Sub Category\",\"Class\",\"Sub Class\"]\n",
    "    if all(col in df.columns for col in new_cols):\n",
    "        print(\"[INFO] Category columns already exist, skipping split\")\n",
    "        return df\n",
    "    \n",
    "    print(\"[INFO] Splitting category column into components...\")\n",
    "    \n",
    "    departments, categories, sub_categories, classes, sub_classes = [], [], [], [], []\n",
    "    \n",
    "    for val in df[\"category\"]:\n",
    "        dept, cat, sub_cat, cls, sub_cls = split_category_path(val)\n",
    "        departments.append(dept)\n",
    "        categories.append(cat)\n",
    "        sub_categories.append(sub_cat)\n",
    "        classes.append(cls)\n",
    "        sub_classes.append(sub_cls)\n",
    "    \n",
    "    df[\"Department\"] = departments\n",
    "    df[\"Category\"] = categories\n",
    "    df[\"Sub Category\"] = sub_categories\n",
    "    df[\"Class\"] = classes\n",
    "    df[\"Sub Class\"] = sub_classes\n",
    "    \n",
    "    return df\n",
    "\n",
    "def coerce_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Convert numeric columns to proper numeric types\"\"\"\n",
    "    for col in NUMERIC_COLS:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\").fillna(0)\n",
    "    return df\n",
    "\n",
    "def drop_old_agg_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Remove existing aggregate columns before recalculation\"\"\"\n",
    "    existing = [c for c in AGG_COLS if c in df.columns]\n",
    "    if existing:\n",
    "        print(f\"[INFO] Removing existing aggregate columns: {existing}\")\n",
    "        df = df.drop(columns=existing)\n",
    "    return df\n",
    "\n",
    "def aggregate_three_months(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calculate 3-month aggregates grouped by specified columns\"\"\"\n",
    "    print(f\"[INFO] Aggregating by: {GROUP_BY_COLUMNS}\")\n",
    "    \n",
    "    agg = (df.groupby(GROUP_BY_COLUMNS, dropna=False)\n",
    "             .agg(\n",
    "                 total_impressions_3m=(\"count\",\"sum\"),\n",
    "                 total_clicks_3m=(\"Clicks\",\"sum\"),\n",
    "                 total_conversions_3m=(\"Conversions\",\"sum\")\n",
    "             ).reset_index())\n",
    "    \n",
    "    # Calculate rates\n",
    "    agg[\"ctr_3m\"] = np.where(\n",
    "        agg[\"total_impressions_3m\"] > 0,\n",
    "        agg[\"total_clicks_3m\"] / agg[\"total_impressions_3m\"],\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    agg[\"cr_3m\"] = np.where(\n",
    "        agg[\"total_impressions_3m\"] > 0,\n",
    "        agg[\"total_conversions_3m\"] / agg[\"total_impressions_3m\"],\n",
    "        0\n",
    "    )\n",
    "    \n",
    "    print(f\"[INFO] Created {len(agg)} aggregate groups\")\n",
    "    return agg\n",
    "\n",
    "def merge_aggregates(df: pd.DataFrame, agg: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Merge aggregate data back to main dataframe\"\"\"\n",
    "    return df.merge(agg, on=GROUP_BY_COLUMNS, how=\"left\")\n",
    "\n",
    "def reorder_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Reorder columns for better readability\"\"\"\n",
    "    preferred = [\n",
    "        \"category\",\"Department\",\"Category\",\"Sub Category\",\"Class\",\"Sub Class\",\n",
    "        \"Brand\",\"search\",\"count\",\"Clicks\",\"Conversions\",\n",
    "        \"clickThroughRate\",\"conversionRate\",\n",
    "        \"total_impressions_3m\",\"total_clicks_3m\",\"total_conversions_3m\",\n",
    "        \"ctr_3m\",\"cr_3m\",\n",
    "        \"averageClickPosition\",\"underperforming\",\n",
    "        \"start_date\",\"end_date\",\"Cluster ID\",\"cluster_id\"\n",
    "    ]\n",
    "    \n",
    "    existing_pref = [c for c in preferred if c in df.columns]\n",
    "    others = [c for c in df.columns if c not in existing_pref]\n",
    "    \n",
    "    return df[existing_pref + others]\n",
    "\n",
    "def write_sheet_in_place(path: str, sheet_name: str, df: pd.DataFrame):\n",
    "    \"\"\"Write dataframe to specific sheet in existing workbook\"\"\"\n",
    "    wb = load_workbook(path)\n",
    "    \n",
    "    if sheet_name not in wb.sheetnames:\n",
    "        raise ValueError(f\"Sheet '{sheet_name}' not found in workbook. Available sheets: {wb.sheetnames}\")\n",
    "    \n",
    "    # Remove old sheet and create new one\n",
    "    ws = wb[sheet_name]\n",
    "    wb.remove(ws)\n",
    "    ws_new = wb.create_sheet(title=sheet_name)\n",
    "    \n",
    "    # Write data\n",
    "    for r in dataframe_to_rows(df, index=False, header=True):\n",
    "        ws_new.append(r)\n",
    "    \n",
    "    wb.save(path)\n",
    "\n",
    "def process_target_sheet():\n",
    "    \"\"\"Main processing function\"\"\"\n",
    "    if not os.path.exists(FILE_PATH):\n",
    "        raise FileNotFoundError(f\"File not found: {FILE_PATH}\")\n",
    "\n",
    "    # Create backup once\n",
    "    if BACKUP_BEFORE_OVERWRITE:\n",
    "        backup_path = FILE_PATH.replace(\".xlsx\", \"_backup_step3.xlsx\")\n",
    "        if not os.path.exists(backup_path):\n",
    "            shutil.copy2(FILE_PATH, backup_path)\n",
    "            print(f\"[INFO] Backup created: {backup_path}\")\n",
    "        else:\n",
    "            print(f\"[INFO] Backup already exists: {backup_path}\")\n",
    "\n",
    "    # Load target sheet\n",
    "    try:\n",
    "        df = pd.read_excel(FILE_PATH, sheet_name=TARGET_SHEET)\n",
    "        print(f\"[INFO] Loaded {len(df)} rows from sheet '{TARGET_SHEET}'\")\n",
    "    except ValueError as e:\n",
    "        raise RuntimeError(f\"Cannot load sheet '{TARGET_SHEET}': {e}\")\n",
    "\n",
    "    # Process data\n",
    "    ensure_core(df)\n",
    "    df = coerce_numeric(df)\n",
    "    df = add_category_columns(df)\n",
    "    df = drop_old_agg_columns(df)\n",
    "    \n",
    "    # Remove any unwanted columns\n",
    "    unwanted_cols = [\"Index_language\"]\n",
    "    for col in unwanted_cols:\n",
    "        if col in df.columns:\n",
    "            df = df.drop(columns=[col])\n",
    "            print(f\"[INFO] Removed column: {col}\")\n",
    "    \n",
    "    # Calculate aggregates\n",
    "    agg = aggregate_three_months(df)\n",
    "    df = merge_aggregates(df, agg)\n",
    "    df = reorder_columns(df)\n",
    "    \n",
    "    # Write back to file\n",
    "    write_sheet_in_place(FILE_PATH, TARGET_SHEET, df)\n",
    "    \n",
    "    print(f\"[DONE] Sheet '{TARGET_SHEET}' updated in workbook: {FILE_PATH}\")\n",
    "    print(f\"[INFO] Final dataset has {len(df)} rows and {len(df.columns)} columns\")\n",
    "    \n",
    "    # Show sample of new columns\n",
    "    if len(df) > 0:\n",
    "        print(\"\\n[INFO] Sample of new aggregate columns:\")\n",
    "        sample_cols = [\"search\",\"total_impressions_3m\",\"total_clicks_3m\",\"ctr_3m\",\"cr_3m\"]\n",
    "        available_cols = [c for c in sample_cols if c in df.columns]\n",
    "        print(df[available_cols].head(3).to_string(index=False))\n",
    "\n",
    "# Execute\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        process_target_sheet()\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ad9170-3706-47fd-a57b-47f079889602",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
